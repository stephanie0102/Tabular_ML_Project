# ğŸ“š ä»£ç é˜…è¯»æŒ‡å— - Tabular ML Project

## ğŸ¯ é¡¹ç›®æ ¸å¿ƒæ¦‚å¿µ

**Research Question**: Does a lightweight gradient boosting model (LightGBM) outperform a pretrained transformer-based baseline (TabPFN 2.5) across multiple heterogeneous tabular datasets?

**Research Question Option**: Does a lightweight tree-based models (LightGBM/XGBoost) outperform a pretrained transformer-based baseline (TabPFN 2.5) across multiple heterogeneous tabular datasets?


**ç­”æ¡ˆ**: **YES!** 
- LightGBMå¹³å‡å‡†ç¡®ç‡ï¼š**0.9143** vs TabPFN baselineï¼š**0.8752** (+4.5% æå‡)
- ç‰¹åˆ«æ˜¯åœ¨HELOCæ•°æ®é›†ä¸Šæ”¹è¿›æ˜¾è‘—ï¼š**0.8931 vs 0.7734** (+15.5% æå‡)

**æ ¸å¿ƒåˆ›æ–°ç‚¹**: 
- è®¾è®¡äº†ä¸€ä¸ª **dataset-agnosticï¼ˆæ•°æ®é›†æ— å…³ï¼‰** çš„ç»Ÿä¸€Pipeline
- é’ˆå¯¹TabPFNçš„50000æ ·æœ¬é™åˆ¶ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆå¦‚HIGGSçš„175kæ ·æœ¬ï¼‰
- è¯æ˜äº†è½»é‡çº§æ ‘æ¨¡å‹åœ¨è¡¨æ ¼æ•°æ®ä¸Šå¯ä»¥è¶…è¶Šå¤§å‹é¢„è®­ç»ƒTransformer

---

## ğŸ“Š é¡¹ç›®ç»“æ„ä¸æ•°æ®æµ

```
æ•°æ®æµå‘ï¼š
run.py â†’ train.py â†’ data_utils.py â†’ models_tabular.py â†’ predict.py â†’ Kaggleæäº¤
         â†“
    [æ•°æ®åŠ è½½] â†’ [é¢„å¤„ç†] â†’ [æ¨¡å‹è®­ç»ƒ] â†’ [éªŒè¯è¯„ä¼°] â†’ [ç”Ÿæˆé¢„æµ‹] â†’ [åˆå¹¶æäº¤]
```

---

## ğŸ” å»ºè®®çš„é˜…è¯»é¡ºåº

### 1ï¸âƒ£ **å…ˆè¯»ï¼šrun.py**ï¼ˆå…¥å£è„šæœ¬ï¼‰
**ä½ç½®**: æ ¹ç›®å½• `/run.py`  
**ä½œç”¨**: æ•´ä¸ªPipelineçš„ä¸»å…¥å£ï¼Œåè°ƒè®­ç»ƒå’Œé¢„æµ‹

**å…³é”®å‡½æ•°**:
```python
def run_full_pipeline(model_type="lgbm", use_cv=True, ...):
    # Step 1: æ•°æ®æ¦‚è§ˆ
    # Step 2: è®­ç»ƒæ‰€æœ‰æ•°æ®é›†
    # Step 3: ç”Ÿæˆé¢„æµ‹å¹¶ä¿å­˜æäº¤æ–‡ä»¶
```

**å¦‚ä½•è¿è¡Œ**:
```bash
python run.py                    # é»˜è®¤ä½¿ç”¨LightGBM
python run.py --model xgb        # ä½¿ç”¨XGBoost
python run.py --model rf         # ä½¿ç”¨Random Forest
python run.py --no-cv            # è·³è¿‡äº¤å‰éªŒè¯ï¼ˆæ›´å¿«ï¼‰
```

---

### 2ï¸âƒ£ **æ ¸å¿ƒ1ï¼šsrc/data_utils.py**ï¼ˆæ•°æ®å¤„ç†å±‚ï¼‰
**ä½œç”¨**: å®ç° **dataset-agnostic input layer**ï¼Œç»Ÿä¸€ä¸‰ä¸ªæ•°æ®é›†çš„æ¥å£

#### è®¾è®¡ç†å¿µï¼š
æ¯ä¸ªæ•°æ®é›†éƒ½æœ‰ä¸€ä¸ªç‹¬ç«‹çš„Loaderç±»ï¼Œä½†éƒ½ç»§æ‰¿è‡ªåŒä¸€ä¸ª `DataLoader` åŸºç±»ï¼Œæä¾›ç»Ÿä¸€æ¥å£ï¼š
- `load_train_data()` â†’ è¿”å› (X, y, feature_columns)
- `load_test_data()` â†’ è¿”å› (X, feature_columns)

#### ä¸‰ä¸ªæ•°æ®é›†çš„ç‰¹ç‚¹å¤„ç†ï¼š

**CovTypeDataLoader** (æ£®æ—è¦†ç›–ç±»å‹)
```python
- 55ä¸ªç‰¹å¾ï¼Œæ— ç¼ºå¤±å€¼
- 7åˆ†ç±»é—®é¢˜ï¼ˆCover_Type: 1-7ï¼‰
- IDä»1å¼€å§‹
- æ•°æ®å·²æ¸…æ´ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
```

**HELOCDataLoader** (ä¿¡ç”¨è¯„åˆ†)
```python
- 23ä¸ªç‰¹å¾
- äºŒåˆ†ç±»ï¼šGood=1, Bad=0
- IDä»3501å¼€å§‹
- âš ï¸ ç¼ºå¤±å€¼å¤„ç†ï¼šè´Ÿæ•°è¡¨ç¤ºç¼ºå¤±
  def _handle_missing(X):
      # è´Ÿæ•° â†’ ç”¨è¯¥åˆ—çš„ä¸­ä½æ•°å¡«å……
      median_val = np.median(valid_values)
```

**HIGGSDataLoader** (å¸Œæ ¼æ–¯ç»è‰²å­)
```python
- 30ä¸ªç‰¹å¾ + 1ä¸ªweightåˆ—
- äºŒåˆ†ç±»ï¼šsignal=1, background=0
- IDä»4547å¼€å§‹
- âš ï¸ ç¼ºå¤±å€¼å¤„ç†ï¼š-999.0è¡¨ç¤ºç¼ºå¤±
  def _handle_missing(X):
      # -999.0 â†’ ç”¨è¯¥åˆ—çš„ä¸­ä½æ•°å¡«å……
```

#### ç»Ÿä¸€æ¥å£å‡½æ•°ï¼š
```python
def get_data_loader(dataset_name):
    """å·¥å‚å‡½æ•°ï¼šæ ¹æ®æ•°æ®é›†åç§°è¿”å›å¯¹åº”çš„Loader"""
    loaders = {
        'covtype': CovTypeDataLoader,
        'heloc': HELOCDataLoader,
        'higgs': HIGGSDataLoader
    }
    return loaders[dataset_name]()
```

**æ ¸å¿ƒä»·å€¼**: 
- âœ… æŠ½è±¡äº†æ•°æ®é›†å·®å¼‚
- âœ… ç»Ÿä¸€çš„é¢„å¤„ç†é€»è¾‘
- âœ… æ˜“äºæ‰©å±•æ–°æ•°æ®é›†

---

### 3ï¸âƒ£ **æ ¸å¿ƒ2ï¼šsrc/models_tabular.py**ï¼ˆæ¨¡å‹å±‚ï¼‰
**ä½œç”¨**: å®ç°å¤šç§è¡¨æ ¼æ•°æ®æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æ¥å£

#### åŸºç±»è®¾è®¡ï¼š
```python
class BaseModel:
    def __init__(self, name, model):
        self.name = name
        self.model = model  # åŒ…è£…sklearn/lgbm/xgbæ¨¡å‹
        
    def fit(X, y):           # è®­ç»ƒ
    def predict(X):          # é¢„æµ‹
    def predict_proba(X):    # æ¦‚ç‡é¢„æµ‹
    def cross_validate(X, y, cv=5):  # äº¤å‰éªŒè¯
```

#### å®ç°çš„æ¨¡å‹ï¼š

**1. TabPFNModel (Baseline - é¢„è®­ç»ƒæ¨¡å‹)**
```python
- æ¥æºï¼šHuggingFace Pre-trained Transformer
- ç‰ˆæœ¬ï¼šTabPFN 2.5
- âš ï¸ é™åˆ¶ï¼šè®­ç»ƒæ ·æœ¬æ•°å¿…é¡» â‰¤ 50,000
- ç‰¹ç‚¹ï¼šä¸éœ€è¦è¶…å‚æ•°è°ƒä¼˜ï¼Œå¼€ç®±å³ç”¨
- Kaggleå¾—åˆ†ï¼š0.95085
```

**2. LightGBMModel (æˆ‘ä»¬çš„ä¸»è¦æ¨¡å‹)**
```python
- æ ‘æ¨¡å‹ï¼Œé€‚åˆè¡¨æ ¼æ•°æ®
- è¶…å‚æ•°ï¼š
  n_estimators=500
  learning_rate=0.05
  max_depth=-1
  num_leaves=31
  class_weight="balanced"  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
- Kaggleå¾—åˆ†ï¼š0.95180ï¼ˆç•¥ä¼˜äºbaselineï¼‰
- å¹³å‡éªŒè¯å‡†ç¡®ç‡ï¼š0.9143
```

**3. XGBoostModel**
```python
- å¦ä¸€ä¸ªå¼ºå¤§çš„æ ‘æ¨¡å‹
- è¶…å‚æ•°ï¼š
  n_estimators=500
  learning_rate=0.05
  max_depth=6
- å¹³å‡éªŒè¯å‡†ç¡®ç‡ï¼š0.9279ï¼ˆæœ€å¥½ï¼ï¼‰
```

**4. RandomForestModel**
```python
- ç®€å•çš„æ ‘æ¨¡å‹baseline
- è¶…å‚æ•°ï¼š
  n_estimators=200
  class_weight="balanced"
```

**5. EnsembleModel (é›†æˆå­¦ä¹ )**
```python
class EnsembleModel:
    def __init__(self, models, voting="soft"):
        # ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹
        # voting="soft": åŸºäºæ¦‚ç‡çš„è½¯æŠ•ç¥¨
```

#### æ¨¡å‹è·å–å‡½æ•°ï¼š
```python
def get_model(model_type, **kwargs):
    """å·¥å‚å‡½æ•°ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è¿”å›æ¨¡å‹å®ä¾‹"""
    models = {
        'baseline': TabPFNModel,
        'tabpfn': TabPFNModel,
        'lgbm': LightGBMModel,
        'xgb': XGBoostModel,
        'rf': RandomForestModel,
        # ... è¿˜æœ‰LR, MLP, SVMç­‰
    }
```

---

### 4ï¸âƒ£ **æ ¸å¿ƒ3ï¼šsrc/train.py**ï¼ˆè®­ç»ƒæµç¨‹ï¼‰
**ä½œç”¨**: è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½

#### ä¸»è¦å‡½æ•°ï¼š

**train_single_dataset()**
```python
def train_single_dataset(dataset_name, model_type="lgbm", 
                         use_cv=True, cv_folds=5, ...):
    # 1. åŠ è½½æ•°æ®
    loader = get_data_loader(dataset_name)
    X_train, y_train, feature_cols = loader.load_train_data()
    
    # 2. TabPFNç‰¹æ®Šå¤„ç†ï¼šæŠ½æ ·åˆ°50k
    if model_type in {"baseline", "tabpfn"}:
        if X_train.shape[0] > 50000:
            X_train, y_train = downsample(50000)
    
    # 3. åŠ è½½æœ€ä½³è¶…å‚æ•°
    best_params = get_best_params_per_dataset()
    
    # 4. æ„å»ºæ¨¡å‹
    model = get_model(model_type, **params)
    
    # 5. äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
    if use_cv:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    
    # 6. è®­ç»ƒ/éªŒè¯åˆ†å‰²
    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, 
                                                  test_size=0.15)
    
    # 7. è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # 8. è¯„ä¼°
    val_pred = model.predict(X_val)
    val_accuracy = accuracy_score(y_val, val_pred)
    
    # 9. ä¿å­˜æ¨¡å‹
    pickle.dump(model, f"{dataset_name}_{model_type}_model.pkl")
    
    return {"model": model, "val_accuracy": val_accuracy, ...}
```

**train_all_datasets()**
```python
def train_all_datasets(model_type="lgbm", ...):
    results = {}
    for dataset_name in ["covtype", "heloc", "higgs"]:
        result = train_single_dataset(dataset_name, model_type, ...)
        results[dataset_name] = result
    
    # æ‰“å°æ±‡æ€»
    print("TRAINING SUMMARY")
    for name, result in results.items():
        print(f"{name.upper()}: Validation Accuracy = {result['val_accuracy']:.4f}")
    
    return results
```

---

### 5ï¸âƒ£ **æ ¸å¿ƒ4ï¼šsrc/predict.py**ï¼ˆé¢„æµ‹å’Œæäº¤ï¼‰
**ä½œç”¨**: ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹å¹¶åˆ›å»ºKaggleæäº¤æ–‡ä»¶

#### ä¸»è¦å‡½æ•°ï¼š

**predict_single_dataset()**
```python
def predict_single_dataset(dataset_name, model_type="lgbm", ...):
    # 1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = load_model(dataset_name, model_type)
    
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    loader = get_data_loader(dataset_name)
    _, _, train_feature_cols = loader.load_train_data()  # ç¡®å®šç‰¹å¾åˆ—
    X_test, feature_cols = loader.load_test_data()
    
    # 3. é¢„æµ‹
    predictions = model.predict(X_test)
    
    # 4. åˆ›å»ºæäº¤DataFrame
    id_start = loader.id_start  # CovType=1, HELOC=3501, HIGGS=4547
    submission = pd.DataFrame({
        "ID": range(id_start, id_start + len(predictions)),
        "Prediction": predictions
    })
    
    # 5. ä¿å­˜
    submission.to_csv(f"{dataset_name}_test_submission.csv", index=False)
    
    return submission
```

**predict_all_datasets()**
```python
def predict_all_datasets(model_type="lgbm", save_combined=True, ...):
    # 1. é¢„æµ‹ä¸‰ä¸ªæ•°æ®é›†
    submissions = {}
    for dataset_name in ["covtype", "heloc", "higgs"]:
        sub = predict_single_dataset(dataset_name, model_type, ...)
        submissions[dataset_name] = sub
    
    # 2. åˆå¹¶æˆä¸€ä¸ªæäº¤æ–‡ä»¶ï¼ˆKaggleè¦æ±‚ï¼‰
    if save_combined:
        combined = pd.concat([
            submissions['covtype'],
            submissions['heloc'],
            submissions['higgs']
        ], ignore_index=True)
        
        combined.to_csv("combined_submission.csv", index=False)
        print(f"Combined submission: {len(combined)} predictions")
```

#### Kaggleæäº¤æ–‡ä»¶æ ¼å¼ï¼š
```csv
ID,Prediction
1,2          # CovType: ID 1-3500
2,3
...
3500,5
3501,0       # HELOC: ID 3501-4546
3502,1
...
4546,0
4547,1       # HIGGS: ID 4547-79546
4548,0
...
```

---

### 6ï¸âƒ£ **baseline.py**ï¼ˆBaselineè¿è¡Œå™¨ï¼‰
**ä½œç”¨**: ä¸“é—¨ç”¨äºè¿è¡ŒTabPFN baselineçš„è„šæœ¬

```python
def main():
    # åªè®­ç»ƒbaselineæ¨¡å‹
    if args.train_only:
        train_all_datasets(model_type="baseline", ...)
    
    # åªç”Ÿæˆé¢„æµ‹
    if args.predict_only:
        predict_all_datasets(model_type="baseline", ...)
    
    # é»˜è®¤ï¼šè®­ç»ƒ+é¢„æµ‹
    train_all_datasets(model_type="baseline", ...)
    predict_all_datasets(model_type="baseline", ...)
```

**è¿è¡Œæ–¹å¼**:
```bash
python baseline.py               # è®­ç»ƒ+é¢„æµ‹baseline
python baseline.py --no-cv       # è·³è¿‡äº¤å‰éªŒè¯ï¼ˆæ›´å¿«ï¼‰
python baseline.py --train-only  # åªè®­ç»ƒ
```

---

## ğŸ“Š å®éªŒç»“æœå¯¹æ¯”

### Validation Accuracyï¼ˆéªŒè¯é›†å‡†ç¡®ç‡ï¼‰

| Model     | CovType | HELOC | HIGGS | **Average** |
|-----------|---------|-------|-------|-------------|
| TabPFN    | 0.9869  | 0.7734| 0.8652| **0.8752**  |
| LightGBM  | 0.9682  | 0.8931| 0.8816| **0.9143**  |
| XGBoost   | 0.9881  | 0.8839| 0.9119| **0.9279**  |

### Kaggle Leaderboard Score

| Model     | Score   |
|-----------|---------|
| TabPFN    | 0.95085 |
| LightGBM  | 0.95180 |

**å…³é”®å‘ç°**:
- âœ… LightGBMå’ŒXGBooståœ¨å¹³å‡å‡†ç¡®ç‡ä¸Šæ˜æ˜¾ä¼˜äºTabPFN baseline
- âœ… æˆ‘ä»¬çš„æ¨¡å‹åœ¨HELOCæ•°æ®é›†ä¸Šæ”¹è¿›æœ€å¤§ï¼ˆ0.7734 â†’ 0.8931ï¼‰
- âœ… XGBoostè¡¨ç°æœ€å¥½ï¼Œä½†LightGBMåœ¨Kaggleä¸Šç•¥èƒœ

---

## ğŸ¯ æ ¸å¿ƒè®¾è®¡æ€æƒ³

### 1. **Dataset-Agnostic Input Layerï¼ˆæ•°æ®é›†æ— å…³è¾“å…¥å±‚ï¼‰**
```python
# ç»Ÿä¸€æ¥å£è®¾è®¡
loader = get_data_loader("covtype")  # æˆ– "heloc" æˆ– "higgs"
X, y, feature_cols = loader.load_train_data()  # æ¥å£ä¸€è‡´ï¼
```

**ä¼˜åŠ¿**:
- æ–°å¢æ•°æ®é›†åªéœ€ç»§æ‰¿ `DataLoader` åŸºç±»
- é¢„å¤„ç†é€»è¾‘åœ¨Loaderå†…éƒ¨å°è£…
- è®­ç»ƒä»£ç å®Œå…¨æ•°æ®é›†æ— å…³

### 2. **Unified Training Pipelineï¼ˆç»Ÿä¸€è®­ç»ƒæµç¨‹ï¼‰**
```python
# ç›¸åŒçš„è®­ç»ƒæµç¨‹é€‚ç”¨äºæ‰€æœ‰æ•°æ®é›†
for dataset_name in ["covtype", "heloc", "higgs"]:
    train_single_dataset(dataset_name, model_type="lgbm")
```

### 3. **é’ˆå¯¹TabPFNé™åˆ¶çš„è§£å†³æ–¹æ¡ˆ**
```python
# TabPFNé™åˆ¶ï¼šâ‰¤50kæ ·æœ¬
# æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼š
if model_type == "baseline" and X_train.shape[0] > 50000:
    # åˆ†å±‚æŠ½æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹
    X_train, y_train = stratified_downsample(50000)

# LightGBM/XGBoostæ²¡æœ‰è¿™ä¸ªé™åˆ¶ï¼Œå¯ä»¥ç”¨å…¨éƒ¨æ•°æ®ï¼
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒå¹¶é¢„æµ‹ï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨LightGBMï¼ˆé»˜è®¤ï¼‰
python run.py

# ä½¿ç”¨XGBoost
python run.py --model xgb

# å¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡CVï¼‰
python run.py --no-cv
```

### 2. åªè¿è¡ŒBaseline
```bash
python baseline.py
```

### 3. åˆ†æ­¥è¿è¡Œ
```bash
# æ­¥éª¤1ï¼šè®­ç»ƒ
python src/train.py --dataset covtype --model lgbm
python src/train.py --dataset heloc --model lgbm
python src/train.py --dataset higgs --model lgbm

# æ­¥éª¤2ï¼šé¢„æµ‹
python src/predict.py --dataset covtype --model lgbm
python src/predict.py --dataset heloc --model lgbm
python src/predict.py --dataset higgs --model lgbm
```

---

## ğŸ“ ä»£ç é˜…è¯»æ£€æŸ¥æ¸…å•

å®Œæˆè¿™äº›ä»»åŠ¡åï¼Œä½ å°±å®Œå…¨ç†è§£ä»£ç äº†ï¼š

- [ ] **ç†è§£æ•°æ®æµ**: æ•°æ®å¦‚ä½•ä»CSV â†’ DataLoader â†’ æ¨¡å‹ â†’ é¢„æµ‹
- [ ] **ç†è§£dataset-agnosticè®¾è®¡**: å¦‚ä½•ç”¨ç»Ÿä¸€æ¥å£å¤„ç†ä¸åŒæ•°æ®é›†
- [ ] **ç†è§£ç¼ºå¤±å€¼å¤„ç†**: HELOCçš„è´Ÿæ•°ã€HIGGSçš„-999.0
- [ ] **ç†è§£æ¨¡å‹å°è£…**: BaseModelå¦‚ä½•ç»Ÿä¸€ä¸åŒæ¨¡å‹çš„æ¥å£
- [ ] **ç†è§£è®­ç»ƒæµç¨‹**: äº¤å‰éªŒè¯ â†’ è®­ç»ƒ/éªŒè¯åˆ†å‰² â†’ è®­ç»ƒ â†’ è¯„ä¼°
- [ ] **ç†è§£æäº¤æ–‡ä»¶ç”Ÿæˆ**: IDå¦‚ä½•åˆ†é…ã€å¦‚ä½•åˆå¹¶ä¸‰ä¸ªæ•°æ®é›†
- [ ] **ç†è§£TabPFNé™åˆ¶**: ä¸ºä»€ä¹ˆéœ€è¦æŠ½æ ·ã€å¦‚ä½•æŠ½æ ·
- [ ] **å¯¹æ¯”å®éªŒç»“æœ**: ä¸ºä»€ä¹ˆLightGBMä¼˜äºTabPFN

---

## ğŸ“ é¡¹ç›®äº®ç‚¹ï¼ˆç”¨äºæµ·æŠ¥ï¼‰

### 1. **Research Question**
**Does a lightweight gradient boosting model (LightGBM) outperform a pretrained transformer-based baseline (TabPFN 2.5) across multiple heterogeneous tabular datasets?**

**ç­”æ¡ˆ**: **YES!** 
- å¹³å‡å‡†ç¡®ç‡ä» **0.8752 â†’ 0.9143** (+4.5%)
- HELOCæ•°æ®é›†æ”¹è¿›æœ€æ˜¾è‘—ï¼š**0.7734 â†’ 0.8931** (+15.5%)
- Kaggleæ’è¡Œæ¦œå¾—åˆ†ï¼š**0.95085 â†’ 0.95180**

### 2. **æ ¸å¿ƒåˆ›æ–°**
- âœ… Dataset-agnosticç»Ÿä¸€Pipelineè®¾è®¡
- âœ… æ™ºèƒ½ç¼ºå¤±å€¼å¤„ç†ï¼ˆä¸­ä½æ•°å¡«å……ï¼‰
- âœ… é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡çš„class_weightè°ƒæ•´
- âœ… çªç ´TabPFNçš„50kæ ·æœ¬é™åˆ¶ï¼ˆHIGGSæ•°æ®é›†175kæ ·æœ¬ï¼‰
- âœ… è¯æ˜è½»é‡çº§æ¨¡å‹å¯è¶…è¶Šå¤§å‹é¢„è®­ç»ƒTransformer

### 3. **å®éªŒå¯¹æ¯”**
- Simple Baseline: Logistic Regression
- Complex Baseline: TabPFNï¼ˆé¢„è®­ç»ƒï¼‰
- Our Models: LightGBM, XGBoost
- æœ€ä½³è¡¨ç°: XGBoost (0.9279)

### 4. **è®¡ç®—å¤æ‚åº¦å¯¹æ¯”**ï¼ˆå¾…è¡¥å……ï¼‰
éœ€è¦æ·»åŠ ï¼š
- å‚æ•°æ•°é‡
- è®­ç»ƒæ—¶é—´
- æ¨ç†é€Ÿåº¦
- å†…å­˜å ç”¨

---

## â“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆHIGGSæ•°æ®é›†è¦å•ç‹¬å¤„ç†weightåˆ—ï¼Ÿ**  
A: ç‰©ç†å®éªŒæ•°æ®å¸¦æœ‰äº‹ä»¶æƒé‡ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹ä¸ä½¿ç”¨ï¼Œæ‰€ä»¥åœ¨ `load_train_data()` ä¸­æ’é™¤äº†ã€‚

**Q: ä¸ºä»€ä¹ˆç”¨ä¸­ä½æ•°è€Œä¸æ˜¯å‡å€¼å¡«å……ç¼ºå¤±å€¼ï¼Ÿ**  
A: ä¸­ä½æ•°å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼Œé€‚åˆè¡¨æ ¼æ•°æ®ã€‚

**Q: ä¸ºä»€ä¹ˆLightGBMåœ¨Kaggleä¸Šç•¥ä¼˜äºXGBoostï¼Ÿ**  
A: å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ–è€…XGBoostéœ€è¦æ›´å¤šè°ƒå‚ã€‚

**Q: IDä¸ºä»€ä¹ˆæ˜¯1, 3501, 4547è¿™æ ·ä¸è¿ç»­ï¼Ÿ**  
A: Kaggleè¦æ±‚ï¼Œç”¨äºåŒºåˆ†ä¸åŒæ•°æ®é›†çš„é¢„æµ‹ã€‚

---

## ğŸ“š æ¨èé˜…è¯»é¡ºåºæ€»ç»“

1. **run.py** - ç†è§£æ•´ä½“æµç¨‹
2. **data_utils.py** - ç†è§£æ•°æ®å¤„ç†
3. **models_tabular.py** - ç†è§£æ¨¡å‹å°è£…
4. **train.py** - ç†è§£è®­ç»ƒé€»è¾‘
5. **predict.py** - ç†è§£é¢„æµ‹å’Œæäº¤
6. **baseline.py** - ç†è§£baselineè¿è¡Œ

ç¥ä½ ç†è§£é¡ºåˆ©ï¼ğŸ‰
