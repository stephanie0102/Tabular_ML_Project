"""
Generate Complete Analysis Report

Combines complexity analysis and error analysis into a comprehensive report
suitable for the project poster/presentation.

Usage:
    python scripts/generate_report.py --models lgbm tabpfn
    python scripts/generate_report.py --models lgbm tabpfn --dataset heloc
"""

import os
import sys
import argparse
import pandas as pd
import json

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from analysis.complexity import ComplexityAnalyzer
from analysis.error_analysis import ErrorAnalyzer


def generate_markdown_report(
    complexity_df: pd.DataFrame,
    error_results: dict,
    output_path: str = "results/analysis/ANALYSIS_REPORT.md"
):
    """Generate a Markdown report with all analyses."""
    
    with open(output_path, 'w') as f:
        f.write("# Model Analysis Report\n\n")
        f.write(f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## 1. Computational Complexity Analysis\n\n")
        f.write("### Model Comparison\n\n")
        f.write(complexity_df.to_markdown(index=False))
        f.write("\n\n")
        
        # Calculate key metrics
        if "tabpfn" in complexity_df["model_type"].values:
            baseline = complexity_df[complexity_df["model_type"] == "tabpfn"].iloc[0]
            
            f.write("### Key Findings\n\n")
            
            for _, row in complexity_df.iterrows():
                if row["model_type"] != "tabpfn":
                    f.write(f"#### {row['model_type'].upper()} vs TabPFN\n\n")
                    
                    if row["n_parameters"] > 0 and baseline["n_parameters"] > 0:
                        ratio = row["n_parameters"] / baseline["n_parameters"]
                        f.write(f"- **Parameters**: {ratio:.3f}x ({'more' if ratio > 1 else 'fewer'})\n")
                    
                    if row["train_time"] > 0 and baseline["train_time"] > 0:
                        speedup = baseline["train_time"] / row["train_time"]
                        f.write(f"- **Training Speed**: {speedup:.2f}x ({'faster' if speedup > 1 else 'slower'})\n")
                    
                    if row["val_accuracy"] > 0 and baseline["val_accuracy"] > 0:
                        diff = row["val_accuracy"] - baseline["val_accuracy"]
                        f.write(f"- **Accuracy**: {diff:+.4f} ({row['val_accuracy']:.4f} vs {baseline['val_accuracy']:.4f})\n")
                    
                    f.write("\n")
        
        f.write("## 2. Error Analysis\n\n")
        
        for model_name, error_data in error_results.items():
            if error_data:
                f.write(f"### {model_name.upper()}\n\n")
                f.write(f"- **Dataset**: {error_data['dataset']}\n")
                f.write(f"- **Accuracy**: {error_data['accuracy']:.4f}\n")
                f.write(f"- **Error Rate**: {error_data['error_rate']:.4f}\n")
                f.write(f"- **Total Samples**: {error_data['n_samples']}\n")
                f.write(f"- **Errors**: {error_data['n_errors']}\n\n")
                
                if 'confident_errors' in error_data and len(error_data['confident_errors']) > 0:
                    f.write("#### High-Confidence Errors (Top 3)\n\n")
                    f.write("| Sample | True Label | Predicted | Confidence |\n")
                    f.write("|--------|-----------|-----------|------------|\n")
                    
                    for err in error_data['confident_errors'][:3]:
                        f.write(f"| {err['sample_index']} | {err['true_label']} | {err['predicted_label']} | {err['confidence']:.4f} |\n")
                    
                    f.write("\n")
        
        f.write("## 3. Visualizations\n\n")
        f.write("See `results/analysis/` directory for:\n")
        f.write("- Confusion matrices\n")
        f.write("- Confidence distributions\n")
        f.write("- Error patterns\n\n")
        
        f.write("---\n\n")
        f.write("*This report was automatically generated by the analysis pipeline.*\n")
    
    print(f"\nMarkdown report saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate comprehensive analysis report"
    )
    parser.add_argument(
        "--models",
        nargs="+",
        default=["lgbm", "tabpfn"],
        help="Models to analyze (default: lgbm tabpfn)"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="all",
        help="Dataset to focus on (default: all)"
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("GENERATING COMPREHENSIVE ANALYSIS REPORT")
    print("=" * 70)
    
    # 1. Complexity Analysis
    print("\n[1/3] Running complexity analysis...")
    complexity_analyzer = ComplexityAnalyzer()
    
    if args.dataset == "all":
        complexity_df = complexity_analyzer.generate_summary_report(model_types=args.models)
    else:
        complexity_df = complexity_analyzer.compare_models(
            dataset_name=args.dataset,
            model_types=args.models,
            save_report=True
        )
    
    # 2. Error Analysis
    print("\n[2/3] Running error analysis...")
    error_analyzer = ErrorAnalyzer()
    error_results = {}
    
    datasets_to_analyze = ["covtype", "heloc", "higgs"] if args.dataset == "all" else [args.dataset]
    
    for dataset in datasets_to_analyze:
        for model_type in args.models:
            key = f"{dataset}_{model_type}"
            error_results[key] = error_analyzer.analyze_model_from_metrics(
                dataset_name=dataset,
                model_type=model_type,
                save_report=True
            )
    
    # 3. Generate Report
    print("\n[3/3] Generating markdown report...")
    generate_markdown_report(complexity_df, error_results)
    
    # Print summary
    print("\n" + "=" * 70)
    print("REPORT SUMMARY")
    print("=" * 70)
    print("\nComplexity Analysis:")
    print(complexity_df[["dataset", "model_type", "n_parameters", "train_time", "val_accuracy"]].to_string(index=False))
    
    print("\n" + "=" * 70)
    print("FILES GENERATED")
    print("=" * 70)
    print("\nresults/analysis/")
    print("  ├── complexity_summary.csv")
    print("  ├── ANALYSIS_REPORT.md")
    print("  ├── *_error_analysis.json")
    print("  └── *_confusion.png")
    print("\nresults/metrics/")
    print("  └── *_metrics.json")
    
    print("\n" + "=" * 70)
    print("✓ Analysis complete!")
    print("=" * 70)


if __name__ == "__main__":
    main()
